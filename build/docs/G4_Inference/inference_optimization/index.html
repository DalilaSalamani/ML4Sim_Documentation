<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<title data-react-helmet="true">Inference optimization | ML4Sim</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://your-docusaurus-test-site.com/docs/G4_Inference/inference_optimization"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Inference optimization | ML4Sim"><meta data-react-helmet="true" name="description" content="One of the figures of merit for selecting the best performing ML fast simulation model is the one with the best accuray while keeping the memory footprint as small as possible. Advances in memory optimization techniques allow the task of inference to have a small memory footprint. Employing these techniques can result in few factors of smaller memory footprint than a non optimzed model."><meta data-react-helmet="true" property="og:description" content="One of the figures of merit for selecting the best performing ML fast simulation model is the one with the best accuray while keeping the memory footprint as small as possible. Advances in memory optimization techniques allow the task of inference to have a small memory footprint. Employing these techniques can result in few factors of smaller memory footprint than a non optimzed model."><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://your-docusaurus-test-site.com/docs/G4_Inference/inference_optimization"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/docs/G4_Inference/inference_optimization" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/docs/G4_Inference/inference_optimization" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.7069df34.css">
<link rel="preload" href="/assets/js/runtime~main.71701159.js" as="script">
<link rel="preload" href="/assets/js/main.f9c432a9.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.png" alt="Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">ML4Sim</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Get started</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/DalilaSalamani/ML4Sim_Documentation.git" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Full and fast simulation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ml_fastsim">Generative modeling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ml_workflow">Machine learning workflow</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/ML_Model/training">Generative models for fast simulation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/G4_Inference/from_training_to_inference">Inference integration in Geant4</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/G4_Inference/from_training_to_inference">From ML training to Geant4 inference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/G4_Inference/G4_examples">Geant4 examples</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/G4_Inference/inference_optimization">Inference optimization</a></li></ul></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Inference optimization</h1><p>One of the figures of merit for selecting the best performing ML fast simulation model is the one with the best accuray while keeping the memory footprint as small as possible. Advances in memory optimization techniques allow the task of inference to have a small memory footprint. Employing these techniques can result in few factors of smaller memory footprint than a non optimzed model.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="optimization-with-onnxruntime">Optimization with ONNXRuntime<a class="hash-link" href="#optimization-with-onnxruntime" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="graph-optimization-in-onnxruntime">Graph optimization in ONNXRuntime<a class="hash-link" href="#graph-optimization-in-onnxruntime" title="Direct link to heading">â€‹</a></h3><p>Graph optimizations are graph-level transformations. ONNXRuntime provides various graph optimizations to improve model performance.Among those: </p><ul><li>Basic Graph Optimizations : which perform a removal of redundant nodes and redundant computations</li><li>Extended Graph Optimizations: which fuse nodes</li></ul><p>Graph optimizations can be performed either in:</p><ul><li>Online mode: where the optimizations are done before the inference. </li><li>Offline mode: the runtime saves the optimized graph to disk. </li></ul><p>ONNX Runtime provides Python, C#, C++, and C APIs to enable different optimization levels and to choose between offline vs. online mode.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="quantization-in-onnx-runtime">Quantization in ONNX Runtime<a class="hash-link" href="#quantization-in-onnx-runtime" title="Direct link to heading">â€‹</a></h3><p><a href="https://onnxruntime.ai/docs/performance/quantization.html" target="_blank" rel="noopener noreferrer">Quantization in ONNX Runtime</a> refers to 8 bit linear quantization. Floating point real values are mapped to an 8 bit quantization space.</p><p>The table below shows the disk space and the memory footprint of the model using the quantization.</p><table><thead><tr><th></th><th>Raw model</th><th>Quantized model</th></tr></thead><tbody><tr><td>Disk space (MB)</td><td>551</td><td>139</td></tr><tr><td>Resident memory (MB)</td><td>2265.34</td><td>650.414</td></tr><tr><td>Virtual memory(MB)</td><td>3205.26</td><td>1339.22</td></tr></tbody></table><p>Graph optimization of a quantized model in an online mode would give:</p><table><thead><tr><th></th><th>Basic optimization</th><th>Extended mooptimizationdel</th></tr></thead><tbody><tr><td>Resident memory (MB)</td><td>650.414</td><td>555.828</td></tr><tr><td>Virtual memory (MB)</td><td>1339.22</td><td>1073.21</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_mojV" id="model-profiling">Model profiling<a class="hash-link" href="#model-profiling" title="Direct link to heading">â€‹</a></h3><p>Using ONNXRuntime, we can enable model execution profiling by setting this in the inference session options. This saves a JSON file which can be used by trace event profiling tools such as chrome for tracing, in order to analyze the execution for example to refine and check time and effect of sequential or parallel execution.</p><p>The image below shows the execution profile of running a single shower simulation event. We can see there are three sequential steps:</p><ul><li>Model loading</li><li>Session initialization : doing graph optim exe when the model is loaded</li><li>Inference running  </li></ul><p><img src="/assets/images/ONNX_Profile_1-a966a8eba00f9ce2292ff03bedc542c2.png" width="1990" height="254"></p><p>If we zoom into the inference running, we can see the model run time, the sequential executtor and all the set of operations used by the model such as the <strong>Relu</strong> activation function (red circle), where the tool can provide us more information such as the wall duration (0.007ms) and the provider which is a CPU executor.</p><p><img src="/assets/images/ONNX_Profile_2-2c12f324c24a5528fc630f27452b7df7.png" width="1758" height="794"></p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Sequential executor </h5></div><div class="admonition-content"><p>This means that the graph is not solved in a parallel way but sequentially </p></div></div><p>To read more:</p><ul><li>Graph Optimizations in ONNX Runtime, <a href="https://onnxruntime.ai/docs/performance/graph-optimizations.html" target="_blank" rel="noopener noreferrer">URL</a></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/DalilaSalamani/ML4Sim_Documentation.git/docs/G4_Inference/inference_optimization.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/G4_Inference/G4_examples"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Geant4 examples</div></a></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#optimization-with-onnxruntime" class="table-of-contents__link toc-highlight">Optimization with ONNXRuntime</a><ul><li><a href="#graph-optimization-in-onnxruntime" class="table-of-contents__link toc-highlight">Graph optimization in ONNXRuntime</a></li><li><a href="#quantization-in-onnx-runtime" class="table-of-contents__link toc-highlight">Quantization in ONNX Runtime</a></li><li><a href="#model-profiling" class="table-of-contents__link toc-highlight">Model profiling</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Get started</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://indico.cern.ch/category/13860/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>ML4Sim<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://mattermost.web.cern.ch/ml4sim/channels/town-square" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Mattermost<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/DalilaSalamani/ML4Sim_Documentation.git" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022, CERN</div></div></div></footer></div>
<script src="/assets/js/runtime~main.71701159.js"></script>
<script src="/assets/js/main.f9c432a9.js"></script>
</body>
</html>