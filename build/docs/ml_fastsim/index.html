<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<title data-react-helmet="true">Generative modeling | ML4Sim</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://your-docusaurus-test-site.com/docs/ml_fastsim"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Generative modeling | ML4Sim"><meta data-react-helmet="true" name="description" content="Generative models combines deep learning with statistical inference and probabilistic modeling. They aim to learn the process by which data are generated according to some true, unknown distribution representd by a finite number of observations distributed according to it. The advantage of using these models is the ability to create new samples from the underlying distribution. In the literature, there are many variants of these models such as Generative Adversarial Networks and Variational Autoencoders."><meta data-react-helmet="true" property="og:description" content="Generative models combines deep learning with statistical inference and probabilistic modeling. They aim to learn the process by which data are generated according to some true, unknown distribution representd by a finite number of observations distributed according to it. The advantage of using these models is the ability to create new samples from the underlying distribution. In the literature, there are many variants of these models such as Generative Adversarial Networks and Variational Autoencoders."><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://your-docusaurus-test-site.com/docs/ml_fastsim"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/docs/ml_fastsim" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/docs/ml_fastsim" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.7069df34.css">
<link rel="preload" href="/assets/js/runtime~main.71701159.js" as="script">
<link rel="preload" href="/assets/js/main.f9c432a9.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.png" alt="Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">ML4Sim</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Get started</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/DalilaSalamani/ML4Sim_Documentation.git" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Full and fast simulation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/docs/ml_fastsim">Generative modeling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ml_workflow">Machine learning workflow</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/ML_Model/training">Generative models for fast simulation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/G4_Inference/from_training_to_inference">Inference integration in Geant4</a></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Generative modeling</h1><p>Generative models combines deep learning with statistical inference and probabilistic modeling. They aim to learn the process by which data are generated according to some true, unknown distribution representd by a finite number of observations distributed according to it. The advantage of using these models is the ability to create new samples from the underlying distribution. In the literature, there are many variants of these models such as Generative Adversarial Networks and Variational Autoencoders.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)<a class="hash-link" href="#generative-adversarial-networks-gans" title="Direct link to heading">â€‹</a></h3><p>GANs are deep generative learning models in which two non-cooperative networks define its architecture: a generator and a discriminator. The generator is trained to produce samples to confuse the discriminator in distinguishing fake and real samples (drawn from the training data distribution). The discriminator tries to correctly identify the original from the generated (fake) sample. When the two networks converge, the GAN is able to generate data that the trained classifier cannot recognize anymore. </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="variational-autoencoders-vaes">Variational Autoencoders (VAEs)<a class="hash-link" href="#variational-autoencoders-vaes" title="Direct link to heading">â€‹</a></h3><h4 class="anchor anchorWithStickyNavbar_mojV" id="representation-learning">Representation learning<a class="hash-link" href="#representation-learning" title="Direct link to heading">â€‹</a></h4><p><strong>Representation learning</strong> or <strong>information bottleneck learning</strong>, is an information theory technique to derive (infer) intrinsic structure from the data. Over the years, it was used as a data compression tool for object detection and speech recognition. Moreover, it was used in natural language processing to learn a distributed representation for each word, referred to as word embedding. Learning word embeddings can be combined with learning image representations in a way that allows to associate text and images. This approach has been used successfully to build Googleâ€™s image search, exploiting large datasets to map images and queries in the same space. Among the first methods in representation learning is <strong>principal component analysis</strong> proposed by Pearson in 1901, a linear projection of the base feature set to a new feature space where the new features are uncorrelated. Fisher, in 1936, proposed the <strong>linear discriminant analysis</strong> to project a dataset onto a lower-dimensional space with a class-separability (distance between the mean of different classes) in order to avoid overfitting. The introduction of <strong>meaningful representation</strong> with a <strong>variational principle</strong> of the input data appeared first in 1999. Extracting relevance from the data was presented as finding a compressed version of an input x that preserves the information about x using a set of bottleneck code words. In 2014, Kingma and Rezende presented the <strong>stochastic variational algorithm</strong> for inferring and learning from a continuous unobserved or latent space in the presence of intractable posterior distributions. </p><h4 class="anchor anchorWithStickyNavbar_mojV" id="latent-variable-modeling">Latent variable modeling<a class="hash-link" href="#latent-variable-modeling" title="Direct link to heading">â€‹</a></h4><p>Let <strong>D</strong> denote a dataset of N data points. Given D, the goal of a latent variable model is to infer unobserved or latent variable <strong>z</strong> of <strong>m</strong> dimension for every observed variable <strong>x</strong> in D of <strong>n</strong> dimension (where n&gt;m) in order to explain and retrieve hidden structures. <strong>p(z)</strong> is the prior distribution over <strong>z</strong> and the posterior inference is represented by the probability distribution <strong>p(z|x)</strong>. A deep latent variable model denotes a latent variable model whose distributions are parameterized by neural networks. We refer as well to a conditional model p(x,z|c), where <strong>c</strong> is a condition (in our case this condition can be the energy of the primary particle). The goal is to learn the generative distribution of <strong>x</strong> from <strong>z</strong>, i.e., <strong>p(x|z)</strong>. One assumption in this model is that the prior <strong>p(z)</strong> is known. Set p(z) to be a unit Gaussian. A good generative model would assign high probabilities to observed x, i.e., learning a good p(x|z) is equivalent to maximizing the probability of the observed data p(x). </p><p>In the area of unsupervised deep learning, combing the idea of representation learning with latent variable models results in having the autoencoder act as a generative model. VAEs are autoencoders designed with a prior on the latent space. </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="autoencoders-and-variational-autoencoders">Autoencoders and variational autoencoders<a class="hash-link" href="#autoencoders-and-variational-autoencoders" title="Direct link to heading">â€‹</a></h3><p>An autoencoder is a neural network trained to reconstruct its input from a latent representation of this input. If the representation has a lower dimension than the input, the model can be used for dimensionality reduction and feature learning. In this case, it is called an <strong>undercomplete autoencoder</strong>. The autoencoder concept has evolved over the years with neural networks. The motivation behind autoencoders is related to learning low dimensional representations. One of the examples of its efficient usage is to derive/retrieve information in a query database. Autoencoders as a semantic hashing approach can be used in the way they learn a reduced, binary representation, then all database entries can be stored in a hash table that maps representation vectors to entries. This hash table allows us to perform information retrieval by returning all database entries that have the same binary code as the query. When the representation has a greater dimension than the input, the model is called <strong>overcomplete autoencoder</strong>. It learns to copy the input  without learning useful features about the data distribution. Using a regularizer in this case avoids limiting the model&#x27;s capacity. This consists of using a loss function to learn other features, such as the sparsity of the representation, rather than only the copy function. Autoencoders are also algorithms that learn a manifold of the data or the structure of the manifold. A manifold is a region where the data is represented as connected points associated within a neighborhood.</p><p>Variational inference models the true distribution <strong>p(x|z)</strong> using a simpler parametric distribution <strong>q<sub>Ï†</sub>(x|z)</strong>. This modeling of q<!-- -->Ï†<!-- -->(x|z) refers to the encoder part of the VAE, also called the inference model, and the parameters <strong>Ï†</strong> are called <strong>variational parameters</strong>. The distribution q<sub>Ï†</sub>(x|z) can be parametrized using neural networks and therefore <!-- -->Ï†<!-- -->parameters represent the weights and biases of the neural network. Applying variational inference consists of first choosing a family of distributions over the latent variables. Then the optimization procedure consists of finding the set of parameters to best approximate the posterior distribution using the Kulback-Leibler divergence. </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="vae-for-fast-shower-simulation-fastv">VAE for fast shower simulation (FastV)<a class="hash-link" href="#vae-for-fast-shower-simulation-fastv" title="Direct link to heading">â€‹</a></h3><p>The VAE model used in the next pages is composed of two stacked deep neural networks acting as encoder and decoder. The encoder learns a mapping from the input space to an unobserved or latent space in which a lower dimensional representation of the full simulation is learned. The decoder learns the inverse mapping, thus reconstructing the original input from this latent representation. The encoded distributions are constrained to be Gaussian distributions and the encoder is tasked to return the mean and the covariance matrix that describe those distributions. The loss function that is optimized during the training of the VAE is composed of a regularization loss to minimize the <strong>Kulback-Leibler divergence</strong> between encoded distributions and prior Gaussian distributions, and a <strong>reconstruction loss</strong> to minimize the error by computing the binary cross-entropy between the input and its reconstruction version using the latent representation. </p><p>To read more: </p><ul><li>Principal component analysis : Pearson, K. On lines and planes of closest fit to systems ofpoints in space.Philosophical Magazine, 1901.</li><li>Linear discriminant analysis: Fisher,  Ronald A. The use of multiple measurements intaxonomic problems.Annals of eugenics, 7(2):179â€“188,193.</li><li>Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014, June). Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning (pp. 1278-1286). PMLR.</li><li>Kingma, D. P., &amp; Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.</li></ul><p>To read more about generative models applied in HEP for fast detector simulation:</p><ul><li>ATLAS Collaboration (2018). Deep generative models for fast shower simulation in ATLAS. ATL-SOFT-PUB-2018-001. <a href="https://cds.cern.ch/record/2630433" target="_blank" rel="noopener noreferrer">CDS</a>.</li><li>Paganini, M., de Oliveira, L., &amp; Nachman, B. (2018). CaloGAN: Simulating 3D high energy particle showers in multilayer electromagnetic calorimeters with generative adversarial networks. Physical Review D, 97(1), 014021.</li><li>Erdmann, M., Glombitza, J., &amp; Quast, T. (2019). Precise simulation of electromagnetic calorimeter showers using a Wasserstein Generative Adversarial Network. Computing and Software for Big Science, 3(1), 1-13.</li><li>Krause, C., &amp; Shih, D. (2021). Caloflow: Fast and accurate generation of calorimeter showers with normalizing flows. arXiv preprint arXiv:2106.05285.</li><li>Bourilkov, D. (2019). Machine and deep learning applications in particle physics. International Journal of Modern Physics A, 34(35), 1930019.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/DalilaSalamani/ML4Sim_Documentation.git/docs/ml_fastsim.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/intro"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Full and fast simulation</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/ml_workflow"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Machine learning workflow</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#generative-adversarial-networks-gans" class="table-of-contents__link toc-highlight">Generative Adversarial Networks (GANs)</a></li><li><a href="#variational-autoencoders-vaes" class="table-of-contents__link toc-highlight">Variational Autoencoders (VAEs)</a></li><li><a href="#autoencoders-and-variational-autoencoders" class="table-of-contents__link toc-highlight">Autoencoders and variational autoencoders</a></li><li><a href="#vae-for-fast-shower-simulation-fastv" class="table-of-contents__link toc-highlight">VAE for fast shower simulation (FastV)</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Get started</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://indico.cern.ch/category/13860/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>ML4Sim<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://mattermost.web.cern.ch/ml4sim/channels/town-square" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Mattermost<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/DalilaSalamani/ML4Sim_Documentation.git" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022, CERN</div></div></div></footer></div>
<script src="/assets/js/runtime~main.71701159.js"></script>
<script src="/assets/js/main.f9c432a9.js"></script>
</body>
</html>