"use strict";(self.webpackChunkml_4_sim=self.webpackChunkml_4_sim||[]).push([[639],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return u}});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),m=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=m(e.components);return r.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},c=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),c=m(n),u=a,h=c["".concat(s,".").concat(u)]||c[u]||d[u]||i;return n?r.createElement(h,o(o({ref:t},p),{},{components:n})):r.createElement(h,o({ref:t},p))}));function u(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var m=2;m<i;m++)o[m]=n[m];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}c.displayName="MDXCreateElement"},6183:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return m},toc:function(){return p},default:function(){return c}});var r=n(7462),a=n(3366),i=(n(7294),n(3905)),o=["components"],l={sidebar_position:1},s="From ML training to Geant4 inference",m={unversionedId:"G4_Inference/from_training_to_inference",id:"G4_Inference/from_training_to_inference",title:"From ML training to Geant4 inference",description:"Once the model is trained, tested and validated, it is then deployed in a production framework. ML deployment involves placing this ML model into an environment where it can perform inference for fast shower simulation. This environment is Geant4.",source:"@site/docs/G4_Inference/from_training_to_inference.md",sourceDirName:"G4_Inference",slug:"/G4_Inference/from_training_to_inference",permalink:"/docs/G4_Inference/from_training_to_inference",editUrl:"https://github.com/DalilaSalamani/ML4Sim_Documentation.git/docs/G4_Inference/from_training_to_inference.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Model optimization",permalink:"/docs/ML_Model/optimization"},next:{title:"Geant4 examples",permalink:"/docs/G4_Inference/G4_examples"}},p=[{value:"Inference libraries",id:"inference-libraries",children:[{value:"LWTNN",id:"lwtnn",children:[],level:3},{value:"ONNXRuntime",id:"onnxruntime",children:[],level:3},{value:"Comparison between LWTNN and ONNXRuntime",id:"comparison-between-lwtnn-and-onnxruntime",children:[],level:3}],level:2}],d={toc:p};function c(e){var t=e.components,n=(0,a.Z)(e,o);return(0,i.kt)("wrapper",(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"from-ml-training-to-geant4-inference"},"From ML training to Geant4 inference"),(0,i.kt)("p",null,"Once the model is trained, tested and validated, it is then deployed in a production framework. ML deployment involves placing this ML model into an environment where it can perform inference for fast shower simulation. This environment is Geant4. "),(0,i.kt)("p",null,"The ML model trained in a python envirnment needs to be first converted to to a format ready to use in C++. The process of model deployment in Geant4 uses external libraries used for ML inference such as LWTNN and ONNXRuntime. Fast simulation components such as an inference model are implemented. "),(0,i.kt)("h2",{id:"inference-libraries"},"Inference libraries"),(0,i.kt)("h3",{id:"lwtnn"},"LWTNN"),(0,i.kt)("p",null,"Lightweight Trained Neural Network or ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://github.com/lwtnn/lwtnn"},"LWTNN"))," is a C++ library to apply neural networks. It has minimal dependencies: Eigen and Boost. It loads and applies the saved model (as a JSON file) using LWTNN in C++. "),(0,i.kt)("p",null,"After mdoel training, to save it as a format that can be used for inference in C++, it should be saved as two separate files of the architecture (in JSON) and the weights (in HDF5). This operation can be done (in Python) with:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"# save the architecture in a JSON file\nwith open('architecture.json', 'w') as arch_file:\n    arch_file.write(model.to_json())\n# save the weights as an HDF5 file\nmodel.save_weights('weights.h5')\n")),(0,i.kt)("p",null,"After building the LWTNN code available at this ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://github.com/lwtnn/lwtnn"},"link")),", run the ",(0,i.kt)("strong",{parentName:"p"},"kerasfunc2json")," python script (available in lwtnn/converters/) to generate a template file of your functional model input variables by calling:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"$ kerasfunc2json.py architecture.json weights.h5 > inputs.json\n")),(0,i.kt)("p",null,"And run again ",(0,i.kt)("strong",{parentName:"p"},"kerasfunc2json")," script to get your output file that would be used for the inference in C++:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"$ kerasfunc2json.py architecture.json weights.h5 inputs.json > Generator.json\n")),(0,i.kt)("p",null,"The object that will do the computation in this class is a ",(0,i.kt)("strong",{parentName:"p"},"LightweightGraph"),", initialized from Generator.json file. The inference is based on evaluating the graph using an input inference vector."),(0,i.kt)("h3",{id:"onnxruntime"},"ONNXRuntime"),(0,i.kt)("p",null,"Open Neural Network Exchange Runtime or ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://github.com/microsoft/onnxruntime"},"ONNXRuntime"))," is a framework for neural networks inference."),(0,i.kt)("p",null,"After training, to save a model as a format that can be used for inference in C++, for a Keras model for example, first it should be saved as HDF5 file with:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'model.save("model.h5")\n')),(0,i.kt)("p",null,"This model is then converted into an ONNX format using the model converted ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://github.com/onnx/keras-onnx"},"keras2onnx"))," with: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"# Create the Keras model\nkerasModel = tensorflow.keras.models.load_model(\u201cmodel.h5\u201d)\n# Convert Keras model into an ONNX model\nonnxModel = keras2onnx.convert_keras( kerasModel , \u2018name\u2019 )\n# Save the ONNX model\nkeras2onnx.save_model(onnxModel, \u2018Generator.onnx')\n")),(0,i.kt)("p",null,"The inference code should create an ",(0,i.kt)("strong",{parentName:"p"},"environment")," which manages an internal thread pool and creates as well the ",(0,i.kt)("strong",{parentName:"p"},"inference session")," for the model. This session runs the inference using an input vector."),(0,i.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Geant4 inference in C++")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Check in the next page the ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"/docs/G4_Inference/G4_examples"},"Par04 example"))," to see the implementation of the inference in C++ using LWTNN and ONNXRuntime"))),(0,i.kt)("h3",{id:"comparison-between-lwtnn-and-onnxruntime"},"Comparison between LWTNN and ONNXRuntime"),(0,i.kt)("p",null,"The table below summarizes the difference between in the two inference libraries in terms of supported ML libraries, disk space and memory footrpint when using the same model to perform inference in Geant4. "),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null}),(0,i.kt)("th",{parentName:"tr",align:null},"LWTNN"),(0,i.kt)("th",{parentName:"tr",align:null},"ONNXRuntime"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Supported ML libraries"),(0,i.kt)("td",{parentName:"tr",align:null},"Saves only Keras and Sklearn models"),(0,i.kt)("td",{parentName:"tr",align:null},"Saves models from (almost) all libraries")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Supported layers"),(0,i.kt)("td",{parentName:"tr",align:null},"All expect:  CNN, Repeat vector, Reshape."),(0,i.kt)("td",{parentName:"tr",align:null},"All")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Supported Activation functions"),(0,i.kt)("td",{parentName:"tr",align:null},"All except: Selu, PRelu"),(0,i.kt)("td",{parentName:"tr",align:null},"All")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"File format"),(0,i.kt)("td",{parentName:"tr",align:null},"JSON"),(0,i.kt)("td",{parentName:"tr",align:null},"ONNX")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Disk space (MB)"),(0,i.kt)("td",{parentName:"tr",align:null},"195"),(0,i.kt)("td",{parentName:"tr",align:null},"28.3")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Resident memory (MB)"),(0,i.kt)("td",{parentName:"tr",align:null},"4000"),(0,i.kt)("td",{parentName:"tr",align:null},"61")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Virtual memory (MB)"),(0,i.kt)("td",{parentName:"tr",align:null},"4000"),(0,i.kt)("td",{parentName:"tr",align:null},"52")))),(0,i.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Tensorflow models and LWTNN")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"It is also possible to convert a tensorflow model into a Keras model and then use it with LWTNN"))),(0,i.kt)("p",null,"For the training, the detector considered is a homogeneous cylinder of lead (PBWO4). It is segmented along (r,","\u03c6",",z)=(24,24,24) to create a readout geometry in the cylindrical coordinates. The model is conditonned on the energy of the incident particle, where a flat energy range is considered for the training going from 1 GeV to 100 GeV. After training the same model is converted into a JSON file and into ONNX. For reference, the disk space for the weights, saved as hdf5 file, is 28.3 MB and the model's architecture, saved as a JSON file is 5.71 kB. The memory footprint of the model presented in the table represents the evaluation of the ",(0,i.kt)("strong",{parentName:"p"},"inference in C++"),". For ",(0,i.kt)("strong",{parentName:"p"},"LWTNN"),", it represents the memory used to compute the graph. For ",(0,i.kt)("strong",{parentName:"p"},"ONNXRuntime"),", it represents the memory used to run the inference session."))}c.isMDXComponent=!0}}]);